# What I have read 

-Meta Learning

1.[Meta-weight-net: Learning an explicit mapping for sample weighting.](https://papers.nips.cc/paper/8467-meta-weight-net-learning-an-explicit-mapping-for-sample-weighting.pdf) NIPS 2019

2.[Meta Transition Adaptation for Robust Deep Learning with Noisy Labels.](https://arxiv.org/pdf/2006.05697.pdf) 2020

3.[MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation.](https://arxiv.org/pdf/2008.12094.pdf) 2020

4.[Data parameters: A new family of parameters for learning a differentiable curriculum.](http://papers.nips.cc/paper/9289-data-parameters-a-new-family-of-parameters-for-learning-a-differentiable-curriculum.pdf) NIPS 2019

5.[When Does Label Smoothing Help?](https://papers.nips.cc/paper/8717-when-does-label-smoothing-help.pdf) NIPS 2019

-Normalization

1.[Batch normalization: Accelerating deep network training by reducing internal covariate shift.](https://arxiv.org/pdf/1502.03167.pdf) PMLR 2015

2.[Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf) 2016

3.[Instance normalization: The missing ingredient for fast stylization.](https://arxiv.org/pdf/1607.08022.pdf) 2016

4.[Group Normalization.](https://openaccess.thecvf.com/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf) ECCV 2018

5.[Differentiable learning-to-normalize via switchable normalization.](https://arxiv.org/pdf/1806.10779.pdf) ICLR 2019

6.[Weight normalization: A simple reparameterization to accelerate training of deep neural networks.](https://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf) NIPS 2016

7.[Cosine normalization: Using cosine similarity instead of dot product in neural networks.](https://arxiv.org/pdf/1702.05870.pdf) ICANN 2018

The kms guide of office
https://msguides.com/microsoft-software-products/office-2019.html
